---
title: "Project Stage 2"
author: "Francis Surroca"
date: '2022-08-05'
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages}
# Add any packages you want in this chunk:
library(plotly)
```

In this final project, we will use R to evaluate logistic regression models with the objective of building a mortgage application approval/denial classifier. Of interest is modeling the probability that an applicant will be approved or denied as a function of observables to classify applicants as being likely to be approved or denied.

# Importing Data

We downloaded the .csv file hmda.sw.csv from the GitHub repository and loaded it getting initial impressions of the data using the 'head' and 'summary' commands.

```{r}
df_model_data <- read.csv("data\\hmda_sw.csv")
# head(df_model_data)
# summary(df_model_data)
```

# Transform Data

We gave the variables in the dataset intuitive names first referencing the data description and the AER paper.

```{r}
MISSING_OBSERVATION = 999999.4

COLUMN_NAMES <- c(
    'Race', 
    'Marital Status', 
    'Self Employed', 
    'Dept to Income', 
    'Years of Education',
    'Type of Action Taken'
)

COLUMN_MAPPING <- c(
    's13'='Race', 
    's23a'='Marital Status', 
    's27a'='Self Employed', 
    's45'= 'Dept to Income',
    'school'= 'Years of Education',
    's7'='Type of Action Taken'
)

ActionTypes <- c(
    'LOAN_ORIGINATED' = 1,
    'APP_APPROVED_NOT_ACCEPTED' = 2,
    'APP_DENIED' = 3,
    'APP_WITHDRAWN' = 4,
    'FILE_CLOSED' = 5,
    'LOAN_PURCHASED' = 6
)

for (i in seq(1,length(COLUMN_MAPPING))){
  col_name_map <- COLUMN_MAPPING[i]
  col_name <- names(col_name_map)
  print(paste("Renaming column: ", col_name, " to: ", col_name_map))
  names(df_model_data)[names(df_model_data) == col_name] <- col_name_map
}

df_model_data <- subset(df_model_data, select = COLUMN_NAMES)
head(df_model_data)

```

# Clean the Data before Interpretation

a.  Set all MISSING_OBSERVATION values to 0 to make data easier to work with

```{r}
df_model_data$'Years of Education'[which(df_model_data$`Years of Education` == MISSING_OBSERVATION)] <- 0
```

b.  Set type of action to either just approved/not approved

```{r}
df_model_data$'Type of Action Taken'[which(df_model_data$'Type of Action Taken' <= ActionTypes['APP_APPROVED_NOT_ACCEPTED'])] <- 1
df_model_data$'Type of Action Taken'[which(df_model_data$'Type of Action Taken' > ActionTypes['APP_APPROVED_NOT_ACCEPTED'])] <- 0

```

c.  Convert marital status to an integer data type

```{r}
df_model_data$'Marital Status'[which(df_model_data$'Marital Status' == 'M')] <- 1
df_model_data$'Marital Status'[which(df_model_data$'Marital Status' == 'U')] <- 0
df_model_data$'Marital Status'[which(df_model_data$'Marital Status' == 'S')] <- 2
df_model_data$'Marital Status'[which(df_model_data$'Marital Status' == 'NA')] <- -1
df_model_data$'Marital Status' <- as.numeric(df_model_data$`Marital Status`)
```

d.  Get rid of huge outliers in dept to income

```{r}
df_model_data$'Dept to Income'[which(df_model_data$'Dept to Income' > 100.0)] <- mean(df_model_data$'Dept to Income')
```

# Generate Summary Statitics

a.  Get and display summary statistics

```{r}
sum_stats = summary(df_model_data)
print(sum_stats)
```

b.  Generate default histograms

```{r}
par(mfrow = c(2,3))
hist(df_model_data$Race)
hist(df_model_data$`Marital Status`)
hist(df_model_data$`Self Employed`)
hist(df_model_data$`Dept to Income`)
hist(df_model_data$`Years of Education`)
hist(df_model_data$`Type of Action Taken`)
par(mfrow = c(1,1))
```

c.  Frequency Counts by Race

```{r}
table(df_model_data$Race, df_model_data$'Marital Status')
table(df_model_data$Race, df_model_data$`Years of Education`)
table(df_model_data$Race, df_model_data$`Self Employed`)
table(df_model_data$Race, df_model_data$`Dept to Income`)
table(df_model_data$Race, df_model_data$`Type of Action Taken`)
```

```{r}
par(mfrow=c(2,3))
hist(table(df_model_data$Race))
hist(table(df_model_data$'Marital Status'))
hist(table(df_model_data$`Years of Education`))
hist(table(df_model_data$`Self Employed`))
hist(table(df_model_data$`Dept to Income`))
hist(table(df_model_data$`Type of Action Taken`))
par(mfrow=c(1,1))
```
A representative applicant is married, white, not self-employed, with 15 years of education and a dept to income ratio of 25.

2)  Scatterplots for relationships between all quantitative variables

\*\* Sale price against all quantitative variables

```{r}
plot_ly(prop_prices, y = ~sale_def, x = ~area, type = 'scatter')
plot_ly(prop_prices, y = ~sale_def, x = ~area_heated, type = 'scatter')
plot_ly(prop_prices, y = ~sale_def, x = ~bed, type = 'scatter')
plot_ly(prop_prices, y = ~sale_def, x = ~dist_cbd, type = 'scatter')
plot_ly(prop_prices, y = ~sale_def, x = ~dist_lakes, type = 'scatter')
plot_ly(prop_prices, y = ~sale_def, x = ~bath, type = 'scatter')

plot(prop_prices$area_heated, prop_prices$sale_def)
abline(lm(sale_def ~ area_heated, data=prop_prices))

plot(prop_prices$area, prop_prices$sale_def)
abline(lm(sale_def ~ area, data=prop_prices))

plot(prop_prices$bed, prop_prices$sale_def)
abline(lm(sale_def ~ bed, data=prop_prices))

plot(prop_prices$dist_cbd, prop_prices$sale_def)
abline(lm(sale_def ~ dist_cbd, data=prop_prices))

plot(prop_prices$dist_lakes, prop_prices$sale_def)
abline(lm(sale_def ~ dist_lakes, data=prop_prices))

plot(prop_prices$bath, prop_prices$sale_def)
abline(lm(sale_def ~ bath, data=prop_prices))

```

# Summary Statistics

Provide basic summary statistics for univariate analysis. Also, provide the correlation between all the quantitative variables.

```{r}
summary(prop_prices)
cor(prop_prices)

```

# Regression Analysis

Run a regression with all the variables included. Print results of the regression.

```{r}
simple_reg = glm(sale_def ~ bed + bath + area + area_heated + dist_cbd + dist_lakes + pool + lake_front + downtown + luxury, data=prop_prices)
summary(simple_reg) 
coef(simple_reg) 

```

Which of the variables tested significant at the 95% level? Looking at the results and answering outside of the chunk is sufficient.

Answer: Pool tested significant at the 95% confidence level.

## Evaluating the model

As is, are any of the Gauss-Markov assumptions violated? If so, which ones? How can you fix the issues?

Using the plot of the regression model to test for Guass-Markov violations.

The Residuals vs Fitted plot shows that there is increasing variance as price increases indicating bias which violates assumption 1. This plot also show that there is no linearity which is also a violation.

The Q-Q plot does not show a diagnol line indicating that the data is not normally distributed.

```{r}

plot(simple_reg)

```

## New Model

Based off of your findings in the previous section, make changes to the variables, the functional form, etc.

```{r}
# Try removing outliers
normalized <- subset(prop_prices, prop_prices$sale_def < 1000000)

# Use only the most significant variables in the formula
fixed_reg = glm(sale_def ~ bed + area_heated + lake_front + luxury, data=normalized)
summary(fixed_reg) 
coef(fixed_reg) 
plot(fixed_reg)
# Looking at the plots, it seems like a better fit

# Try using log of large values to even the playing field
fixed_reg2 = glm(log(sale_def) ~ bed + log(area_heated) + lake_front + luxury, data=normalized)
summary(fixed_reg2) 
coef(fixed_reg2) 
plot(fixed_reg2)

```

# Prediction

Based on the following inputs, predict the deflated sales price:

-   2 bed
-   2 bath
-   area_heated = 1223
-   area = 9750
-   dist_cbd = 19368
-   dist_lakes = 490
-   no pool

```{r}
# Using basic model
new_house_df <- data.frame(bed=2,bath=2,area_heated=1223,area=9750,dist_cbd=19368,dist_lakes=490,pool=0,lake_front=0,luxury=0)
basic_linear_model <- lm(formula = sale_def ~ bed + bath + area + area_heated + dist_cbd + 
    dist_lakes + pool, data = prop_prices)
predict(basic_linear_model, newdata=new_house_df)

# Now using fixed models
predict(fixed_reg, newdata=new_house_df)

# Need to take e^predication since fixed_reg2 is based on logs
exp(predict(fixed_reg2, newdata=new_house_df))


```
