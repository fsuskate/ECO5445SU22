---
title: "Project Stage 2"
author: "Francis Surroca"
date: '2022-08-05'
output: 
  html_document:
    toc: true
    toc_float: true
    theme: cerulean
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages, echo=FALSE, results='hide', message=FALSE}
# Add any packages you want in this chunk:
library(plotly)
library(pROC)
library(knitr)
library(caret)
library(glmnet)
library(gamlr)
library(rpart)
library(rpart.plot)
library(party)
library(randomForest)
library(e1071)

```

# Overview

![Cover Image](images/cover2.png)

In this final project, we will use R to evaluate logistic regression models with the objective of building a mortgage application approval/denial classifier. Of interest is modeling the probability that an applicant will be approved or denied as a function of observables to classify applicants as being likely to be approved or denied.

# A. Importing and Cleaning the Data

*"The variables in the dataset do not have intuitive names (e.g., the meaning of S3 is unclear). Referencing the data description and the AER paper, identify the qualitative dependent that you will be modeling and the set of co-variates that you intend to include in your various models, and rename the variables so that they have (somewhat) intuitive names. Be certain that the debt-to-income ratio and the race, self-employed, marital status, and education indicator variables are included, among other variables."*

### 1. Load Datasets. 

We downloaded the .csv file hmda.sw.csv from the GitHub repository and loaded it getting initial impressions of the data using the 'head' and 'summary' commands. Convert tibble to data.frame and remove all rows with NAs in them.

```{r}
applications <- read.csv("data\\hmda_sw.csv")
applications <- as.data.frame(applications)
head(applications)
summary(applications)
```

### 2. Perform Transformations.

Next, we transformed the dataset and gave the variables in the dataset intuitive names first referencing the data description and the AER paper. We made sure to replace missing observations with the value of 999999.4 with a reasonable value.

```{r}
MISSING_OBSERVATION = 999999.4

COLUMN_NAMES <- c(
    'Race', 
    'Marital Status', 
    'Self Employed', 
    'Dept to Income', 
    'Years of Education',
    'Type of Action Taken'
)

COLUMN_MAPPING <- c(
    's13'='Race', 
    's23a'='Marital Status', 
    's27a'='Self Employed', 
    's45'= 'Dept to Income',
    'school'= 'Years of Education',
    's7'='Type of Action Taken'
)

ActionTypes <- c(
    'LOAN_ORIGINATED' = 1,
    'APP_APPROVED_NOT_ACCEPTED' = 2,
    'APP_DENIED' = 3,
    'APP_WITHDRAWN' = 4,
    'FILE_CLOSED' = 5,
    'LOAN_PURCHASED' = 6
)

for (i in seq(1,length(COLUMN_MAPPING))){
  col_name_map <- COLUMN_MAPPING[i]
  col_name <- names(col_name_map)
  print(paste("Renaming column: ", col_name, " to: ", col_name_map))
  names(applications)[names(applications) == col_name] <- col_name_map
}

applications <- subset(applications, select = COLUMN_NAMES)
head(applications)

```

### 3. Clean Data

It was necessary to clean the data before Interpretation

#### a. Set all MISSING_OBSERVATION values to 0 to make data easier to work with, replace all NA with a reasonable value

```{r}
applications$'Years of Education'[which(applications$`Years of Education` == MISSING_OBSERVATION)] <- median(applications$`Years of Education`)

# applications[which(is.na(applications), arr.ind = TRUE)] <- 1

# remove all rows with NAs
applications <- na.omit(applications)

```

#### b. Set type of action to a categorical factor and introduce approved column

```{r}
applications$`Type of Action Taken` <- factor(applications$`Type of Action Taken`, 
                                               levels=c(1,2,3), 
                                               labels=c('originated', 'approved_not_accepted', 'denied'))

applications$'Approved' <- 0
applications$'Approved'[applications$'Type of Action Taken' == 'originated' | 
                           applications$'Type of Action Taken' == 'approved_not_accepted'] <- 1
applications$'Approved'[applications$'Type of Action Taken' == 'denied'] <- 0
applications$Approved <- factor(applications$Approved, 
                                 levels=c(0,1), 
                                 labels=c('denied', 'approved'))
```

#### c. Convert race, marital status, and self employed to a categorical factor

```{r}
applications$`Race` <- factor(applications$`Race`,
                                               levels=c(3,5), 
                                               labels=c('black','white'))

applications$`Marital Status` <- factor(applications$`Marital Status`,
                                               levels=c('M','U','S'), 
                                               labels=c('married', 'unmarried', 'seperated'))
# applications[which(is.na(applications$`Marital Status`), arr.ind=TRUE),2] <- 'unmarried' 

applications$`Self Employed` <- factor(applications$`Self Employed`,
                                               levels=c(0,1), 
                                               labels=c('not_self_employed', 'self_employed'))
```

#### d. Introduce new column to categorize applicants as having low, med, high risk DTI. I used the general guideline risk used by lenders found here https://www.investopedia.com/terms/d/dti.asp#toc-why-is-debt-to-income-ratio-important

```{r}
applications$Risk <- applications$'Dept to Income'
applications$'Risk'[applications$'Risk' <= 28] <- 0
applications$'Risk'[applications$'Risk' > 28 & applications$'Risk' < 43] <- 1
applications$'Risk'[applications$'Risk' > 43] <- 2
applications$'Risk' <- factor(applications$Risk, 
                              levels=c(0,1,2),
                              labels=c('low', 'med', 'high'))
# applications[which(is.na(applications$`Risk`), arr.ind=TRUE),8] <- 'med' 
```

There were many variables that we considered not to be significant therefore, the variables that we've chosen to include are the ones that we theorized would be the most significant. TODO: explain each

# B. Generate Summary Statistics

*"Generate summary statistics on the set of variables selected in A, and explain the composition of the sample and of the characteristics of an average (representative) applicant. In the process, you should also generate and histograms and frequency counts on particular variables of interest, which can be referenced in your explanation of the composition of the sample and of a representative applicant."*

### a. Get and display summary statistics

```{r}
head(applications)
sum_stats = summary(applications)
print(sum_stats)
```

### b. Generate default histograms for quantitative columns. Exclude all categorical columns.

```{r}
par(mfrow = c(1,2))
hist(applications$`Dept to Income`, main="Dept to Income")
hist(applications$`Years of Education`, main= "Years of Education")
par(mfrow = c(1,1))
```
We can see from the histogram for Dept to Income that the raw data does not appear to be normally distributed. We will therefore need to normalize it in some manner. Let's try removing rows with greater than 100$ DTI which does not make sense.

#### i. Normalize Dept to Income
```{r}
# applications$`Years of Education Norm` <- log(applications$`Years of Education`)
outliers <- c(which(applications$`Dept to Income` > 100))
applications <- applications[-c(which(applications$`Dept to Income` > 100)), ]

hist(applications$`Dept to Income`, main="Dept to Income")


```
Without the outliers, the DTI now appears to be normally distributed so, we will move on.

### c. Frequency Counts by Race

```{r}
kable(table(applications$Race, applications$'Marital Status', dnn = c('Race', 'Marital Status')))
kable(table(applications$Race, applications$`Years of Education`, dnn = c('Race', 'Years of Education')))
kable(table(applications$Race, applications$`Self Employed`, dnn = c('Race', 'Self Employed')))
#table(applications$Race, applications$`Dept to Income`, dnn = c('Race', 'DTI'))
kable(table(applications$Race, applications$`Risk`, dnn = c('Race', 'Risk')))
kable(table(applications$Race, applications$`Type of Action Taken`, dnn = c('Race', 'Type of Action')))

```

### d. Histograms of quantitative data columns

```{r}
par(mfrow=c(1,2))
hist(applications$`Years of Education`, main = "Years of Education")
hist(applications$`Dept to Income`, main = "Dept to Income")
par(mfrow=c(1,1))
```

Here we discuss a representative sample applicant. A representative applicant is married, white, not self-employed, with 15 years of education and a dept to income ratio of 25 making them low risk applicant of which most were approved.

# C. Logistic Regression Model

*"With the full sample, estimate the logistic regression model, where the deny/approve dummy variable is the response variable and the debt-to-income ratio and the race, self-employed, marital status, and education indicator variables are the co-variates. Graph the ROC curve and calculate the AUC. Also, compute the confusion matrix at alternative cut-off levels, and calculate the classifier sensitivity, specificity, the false-positive rate, the false-negative rate, the model accuracy and error rate to confirm they are the same as those produced by R. Provide a written explanation summarizing the findings."*

### a. Logistic Regression Analysis

Here we run create a logistic regression with response variable being the approved/denied column and the using all other variables as co-variates except for Type of Action Taken which causes the model to not converge.

```{r}
fit.logit <- glm(Approved ~ Risk + Race + 
                  applications$`Self Employed` + 
                  applications$`Marital Status` + 
                  applications$`Years of Education`, 
                data=applications, 
                family = 'binomial')
summary(fit.logit) 
coef(fit.logit) 

```
We see here that High Risk, Race White and Years of Education, all tested very significant. We will try to hone down our model by trimming out Self Employment and Marital Status on the hunch that they are causing noise.

```{r}
fit.logit <- glm(Approved ~ Risk + Race + applications$`Years of Education`, data=applications, family = 'binomial')
summary(fit.logit) 
coef(fit.logit) 

```
This model with just Race, Risk and Year of Education seems to better explain an applicatants changes of being approved. We see that a High Risk applicant has 1.72% less chance of being approved and just being White increases the applicants chance of being approved by 1.3%. 

### b. Train and Validate a Classifier and then Graph the ROC

```{r}
set.seed(1234)

train <- sample(nrow(applications), 0.7*nrow(applications))

applications.train <- applications[train,]
applications.validate <- applications[-train,]

table(applications.train$Approved)
summary(applications.validate$Approved)

# Fit the model on the trained data with variables from the validation dataframe
fit.logit.trained <- glm(Approved ~ Risk + Race + `Years of Education`, data=applications.train, family = 'binomial')

summary(fit.logit.trained)

probability <- predict(fit.logit.trained, applications.validate, type = 'response')

logit.predicted <- factor(probability > 0.75, levels = c(FALSE,TRUE),
                     labels = c("approved","denied"))

logit.perf <- table(applications.validate$Approved, logit.predicted, dnn = c("Actual","Predicted"))

logit.perf

# Graph ROC
applications_roc <- roc(applications.validate$Approved ~ probability, plot = TRUE, print.auc=TRUE)

# Calculate Area Under Curve
auc(applications_roc)
```

### c. Find threshold that maximizes the sensitivity and specificity by pulling it from the ROC object.

```{r}
coords(applications_roc, "best", ret = "threshold")
coords(applications_roc)

# or

threshold <- applications_roc$thresholds[which.max(applications_roc$sensitivities + applications_roc$specificities)]
threshold
```
We see that a threshold of .8967 will maximize specificity. So we will use that going forward.

### d. Use the best threshold

```{r}
logit.predicted.thresh <- factor(probability > threshold, levels = c(FALSE,TRUE),
                     labels = c("approved","denied"))

logit.perf.thresh <- table(applications.validate$Approved, logit.predicted.thresh, dnn = c("Actual","Predicted"))

logit.perf.thresh
```
The confusion matrix shows that our model had many false negatives. It doesn't appear to perform well and is much worse than just taking the average and just approving every applicant which will be correct 88% of the time.

### e. Introduce Performance method from 'R in Action' to get the sensitivity, specificity, the false-positive rate, the false-negative rate, the model accuracy and error rate.

```{r}
performance <- function(title, table, n = 4){
  if(!all(dim(table) == c(2,2)))
    stop("Must be a 2 x 2 table")
  tn = table[1,1]
  fp = table[1,2]
  fn = table[2,1]
  tp = table[2,2]
  sensitivity = tp/(tp+fn)
  specificity = tn/(tn+fp)
  ppp = tp/(tp+fp)
  npp = tn/(tn+fp)
  hitrate = (tp+tn)/(tp+tn+fp+fn)
  result <- paste0("----| ", title, " |----", "\nSensitivity = ", round(sensitivity,n),
                   "\nSpecificity = ",round(specificity,n),
                   "\nPositive Predictive Value = ", round(ppp,n),
                   "\nNegative Predictive Value = ", round(npp,n),
                   "\nAccuracy = ", round(hitrate,n), "\n\n")
  cat(result)
}
```

```{r}
performance("Logit.perf", logit.perf)

performance("Logit.perf.thresh", logit.perf.thresh)

```
The accuracy of our model was below the 88% average at 84%.


### f. The predictive accurracy of the original model was low but the after using the best threshold the predictive accuracy was actually lowered. 

# D. Validation Techniques

*"Next, using 10-fold cross validation, estimate a variety of logistic regression models and evaluate their predictive performance across a range of threshold values in each case. The models can (should) include interaction variables and polynomial terms (e.g., quadratic and cubic variables). Of interest is identifying the model and threshold value that yield the smallest average test mis-classification rate; however, you can also calculate model accuracy and the AUC. Document in a table the performance of the various models using the chosen performance measures."*

### a. Run Gamma-Lasso regression
```{r}
applications <- na.omit(applications)
training_response <- data.matrix(applications$Approved)
training_data_matrix <- data.matrix(applications[ , c('Race','Risk', 'Years of Education')])
result_reg <- gamlr(training_data_matrix, as.factor(training_response),  verb=FALSE, family='binomial')
plot(result_reg)
```

### b. Perform 10 fold cross validation
```{r}
cv.result_reg <- cv.gamlr(training_data_matrix, as.factor(training_response), verb=TRUE,nfold =10)
beta1se <- coef(cv.result_reg) ## 1se rule; see ?cv.gamlr
betamin <- coef(cv.result_reg, select="min") ## min cv selection

par(mfrow=c(1,2))
plot(cv.result_reg)
plot(cv.result_reg$gamlr) ## cv.gamlr includes a gamlr object
par(mfrow=c(1,2))

```
### c. Compare model selections

```{r}
ll <- log(result_reg$lambda) ## the sequence of lambdas
n <- nrow(applications)
par(mfrow=c(1,2))
plot(cv.result_reg)
plot(ll, AIC(result_reg)/n, # scaled by the number of obs (n)
	xlab="log lambda", ylab="IC/n", pch=21, bg="orange")
abline(v=ll[which.min(AIC(result_reg))], col="orange", lty=3)
abline(v=ll[which.min(BIC(result_reg))], col="green", lty=3)
abline(v=ll[which.min(AICc(result_reg))], col="black", lty=3)
points(ll, BIC(result_reg)/n, pch=21, bg="green")
points(ll, AICc(result_reg)/n, pch=21, bg="black")
legend("topleft", bty="n",
	fill=c("black","orange","green"),legend=c("AICc","AIC","BIC"))
```

### d. Perform a stepwise regression.

```{r}
logit.fit.reduced <- step(fit.logit)

```

### f. Attempt to use Decision Trees

```{r}
set.seed(1234)

# make the decision tree with the rpart function
dtree <- rpart(Approved ~ Race + Risk + applications$`Years of Education`, data = applications, method = 'class', parms = list(split = 'information'))

prp(dtree, type = 2, extra = 104,fallen.leaves = T)

```
### g. Try pruning the tree

```{r}
plotcp(dtree)

dtree$cptable

# change the penalty value 'cp' to prune the tree
dtree.pruned <- prune(dtree,cp = 0.18)

prp(dtree.pruned, type = 2, extra = 104,fallen.leaves = T)
```

### h. Try Using a Conditional Inference Tree

```{r}
fit.ctree <- ctree(Approved ~ Race + Risk, data = applications)

plot(fit.ctree)
```

### i. Try Using Random Forests

```{r}
set.seed(1234)

fit.forest <- randomForest(Approved ~ Race + Risk + applications$`Years of Education`, data=applications, na.action=na.roughfix, importance = TRUE)

fit.forest
```

### j. Try Using Support Vector Machines

```{r}
set.seed(1234)

fit.svm <- svm(Approved ~ Race + Risk + applications$`Years of Education`, data=na.omit(applications))

fit.svm
```

### m. Tuning the SVM model

```{r}
set.seed(1234)

tuned <- tune.svm(Approved~ Race + Risk, data = na.omit(applications.train),
                  gamma = 10^(-6:1),
                  cost = 10^(-10:10))
tuned
```

n. Then use the parameters to create the model

```{r}
set.seed(1234)

fit.svm <- svm(Approved ~ Race + Risk, data=applications.train, gamma = 0.01, cost = 1)

fit.svm
```


# E. Identify the Superior Model

"*Of the competing models that you estimated and thresholds that you evaluated, identify the superior model for classification purposes. Re-estimate the model with the full sample of data. Then, graph the ROC, calculate the AUC, and compute the confusion matrix at the threshold level associated with the minimum average test mis-classification rate . Calculate the classifier sensitivity and specificity, the false-positive rate, the false negative rate, the accuracy, and overall mis-classification rate. How well does your superior model perform relative to the model estimated in C? Explain. Note that to do so you will need to calculate the confusion matrix from the estimated model in C at the threshold level.*"

Here, we will examine the performance of the various models that we created in section D.

### a. Get ROC curve for the reduced model.

```{r}

prob_reduced <- predict(logit.fit.reduced, applications, type = 'response')

my_roc_reduced <- roc(applications$Approved ~ prob_reduced, plot = TRUE, print.auc=TRUE)

coords(my_roc_reduced, "best", ret="threshold")

logit.reduced.pred.thresh <- factor(prob_reduced > threshold, levels = c(FALSE,TRUE),
                     labels = c("approved","denied"))

logit.prob_reduced.perf.thresh <- table(applications$Approved, logit.reduced.pred.thresh, dnn = c("Actual","Predicted"))

logit.prob_reduced.perf.thresh
```


### b. Test the Predictive Accuracy of DTree model

```{r}
dtree.pred <- predict(dtree, applications,type = "class")

dtree.perf <- table(applications$Approved, dtree.pred, dnn = c("Actual","Predicted"))

dtree.perf
```


### c. Prediction and Performance of the CTree model

```{r}
ctree.pred <- predict(fit.ctree, applications.validate, type = 'response')

ctree.perf <- table(applications.validate$Approved, ctree.pred, dnn = c("Actual","Predicted"))
```

### d. Inspect the variable importance for the Forest model

```{r}
importance(fit.forest,type = 2)
```

### e. Prediction using the Forest model

```{r}
forest.pred <- predict(fit.forest, applications)

forest.perf <- table(applications$Approved, forest.pred, dnn = c("Actual","Predicted"))

forest.perf
```

### f. Prediction and Performance of SVM

```{r}
svm.pred <- predict(fit.svm,na.omit(applications))

svm.perf <- table(na.omit(applications)$Approved, svm.pred, dnn = c("Actual","Predicted"))

svm.perf
```

### g. Prediction and Performance of Tuned SVM Model

```{r}
svm.pred <- predict(fit.svm,na.omit(applications.validate))

svm.perf <- table(na.omit(applications.validate)$Approved, svm.pred, dnn = c("Actual","Predicted"))

```

### h. Use the results to Select the Best Model

Performance Metrics for all Models

```{r}
performance("Logic", logit.perf)
performance("DTree", dtree.perf)
performance("CTree", ctree.perf)
performance("Forest", forest.perf)
performance("SVM", svm.perf)

```

# F. Summarize Modeling Efforts

*"Upload an error-free and well-organized copy of your program and a report that summarizes your modeling efforts and the discussion/explanation of your findings from B, C, D, and E. Be certain to include a table describing the results from the various models you evaluated, and be certain to include the estimation results from your superior model estimated with the full dataset."*

```{r}
```
