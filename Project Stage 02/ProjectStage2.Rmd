---
title: "Project Stage 2"
author: "Francis Surroca"
date: '2022-08-05'
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r packages}
# Add any packages you want in this chunk:
library(plotly)
```

# Overview

In this final project, we will use R to evaluate logistic regression models with the objective of building a mortgage application approval/denial classifier. Of interest is modeling the probability that an applicant will be approved or denied as a function of observables to classify applicants as being likely to be approved or denied.

# A. Importing and Cleaning the Data

The variables in the dataset do not have intuitive names (e.g., the meaning of S3 is unclear). Referencing the data description and the AER paper, identify the qualitative dependent that you will be modeling and the set of co-variates that you intend to include in your various models, and rename the variables so that they have (somewhat) intuitive names. Be certain that the debt-to-income ratio and the race, self-employed, marital status, and education indicator variables are included, among other variables.

1.  We downloaded the .csv file hmda.sw.csv from the GitHub repository and loaded it getting initial impressions of the data using the 'head' and 'summary' commands.

```{r}
applications <- read.csv("data\\hmda_sw.csv")
head(applications)
summary(applications)
```

2.  Next, we transformed the dataset and gave the variables in the dataset intuitive names first referencing the data description and the AER paper. We made sure to replace missing observations with the value of 999999.4 with a reasonable value.

```{r}
MISSING_OBSERVATION = 999999.4

COLUMN_NAMES <- c(
    'Race', 
    'Marital Status', 
    'Self Employed', 
    'Dept to Income', 
    'Years of Education',
    'Type of Action Taken'
)

COLUMN_MAPPING <- c(
    's13'='Race', 
    's23a'='Marital Status', 
    's27a'='Self Employed', 
    's45'= 'Dept to Income',
    'school'= 'Years of Education',
    's7'='Type of Action Taken'
)

ActionTypes <- c(
    'LOAN_ORIGINATED' = 1,
    'APP_APPROVED_NOT_ACCEPTED' = 2,
    'APP_DENIED' = 3,
    'APP_WITHDRAWN' = 4,
    'FILE_CLOSED' = 5,
    'LOAN_PURCHASED' = 6
)

for (i in seq(1,length(COLUMN_MAPPING))){
  col_name_map <- COLUMN_MAPPING[i]
  col_name <- names(col_name_map)
  print(paste("Renaming column: ", col_name, " to: ", col_name_map))
  names(applications)[names(applications) == col_name] <- col_name_map
}

applications <- subset(applications, select = COLUMN_NAMES)
head(applications)

```

3.  It was necessary to clean the data before Interpretation

<!-- -->

a.  Set all MISSING_OBSERVATION values to 0 to make data easier to work with

```{r}
applications$'Years of Education'[which(applications$`Years of Education` == MISSING_OBSERVATION)] <- 0
```

b.  Set type of action to a categorical factor and introduce approved column

```{r}
applications$`Type of Action Taken` <- factor(applications$`Type of Action Taken`, 
                                               levels=c(1,2,3), 
                                               labels=c('originated', 'approved_not_accepted', 'denied'))

applications$'Approved' <- 0
applications$'Approved'[applications$'Type of Action Taken' == 'originated' | 
                           applications$'Type of Action Taken' == 'approved_not_accepted'] <- 1
applications$'Approved'[applications$'Type of Action Taken' == 'denied'] <- 0
applications$Approved
applications$Approved <- factor(applications$Approved, 
                                 levels=c(0,1), 
                                 labels=c('denied', 'approved'))
head(applications)
```

c.  Convert race, marital status, and self employed to a categorical factor

```{r}
applications$`Race` <- factor(applications$`Race`,
                                               levels=c(3,5), 
                                               labels=c('black','white'))

applications$`Marital Status` <- factor(applications$`Marital Status`,
                                               levels=c('M','U','S'), 
                                               labels=c('married', 'unmarried', 'seperated'))

applications$`Self Employed` <- factor(applications$`Self Employed`,
                                               levels=c(0,1), 
                                               labels=c('not_self_employed', 'self_employed'))
```

d.  Introduce new column to categorize applicants as having low, med, high risk DTI

```{r}
applications$Risk <- applications$'Dept to Income'
applications$'Risk'[applications$'Risk' <= 28] <- 0
applications$'Risk'[applications$'Risk' > 28 & applications$'Risk' < 43] <- 1
applications$'Risk'[applications$'Risk' > 43] <- 2
applications$'Risk' <- factor(applications$Risk, 
                              levels=c(0,1,2),
                              labels=c('low', 'med', 'high'))
```

# B. Generate Summary Statistics

Generate summary statistics on the set of variables selected in A, and explain the composition of the sample and of the characteristics of an average (representative) applicant. In the process, you should also generate and histograms and frequency counts on particular variables of interest, which can be referenced in your explanation of the composition of the sample and of a representative applicant.

a.  Get and display summary statistics

```{r}
sum_stats = summary(applications)
print(sum_stats)
```

b.  Generate default histograms for quantitative columns. Exclude all categorical columns.

```{r}
par(mfrow = c(1,2))
hist(applications$`Dept to Income`)
hist(applications$`Years of Education`)
par(mfrow = c(1,1))
```

c.  Frequency Counts by Race

```{r}
table(applications$Race, applications$'Marital Status', dnn = c('Race', 'Marital Status'))
table(applications$Race, applications$`Years of Education`, dnn = c('Race', 'Years of Education'))
table(applications$Race, applications$`Self Employed`, dnn = c('Race', 'Self Employed'))
#table(applications$Race, applications$`Dept to Income`, dnn = c('Race', 'DTI'))
table(applications$Race, applications$`Risk`, dnn = c('Race', 'Risk'))
table(applications$Race, applications$`Type of Action Taken`, dnn = c('Race', 'Type of Action'))

```

```{r}
par(mfrow=c(2,3))
hist(table(applications$Race))
hist(table(applications$'Marital Status'))
hist(table(applications$`Years of Education`))
hist(table(applications$`Self Employed`))
hist(table(applications$`Dept to Income`))
hist(table(applications$`Type of Action Taken`))
par(mfrow=c(1,1))
```

Here we discuss a representative sample applicant. A representative applicant is married, white, not self-employed, with 15 years of education and a dept to income ratio of 25.

# C. Logistic Regression Model

With the full sample, estimate the logistic regression model, where the deny/approve dummy variable is the response variable and the debt-to-income ratio and the race, self-employed, marital status, and education indicator variables are the co-variates. Graph the ROC curve and calculate the AUC. Also, compute the confusion matrix at alternative cut-off levels, and calculate the classifier sensitivity, specificity, the false-positive rate, the false-negative rate, the model accuracy and error rate to confirm they are the same as those produced by R. Provide a written explanation summarizing the findings.

########################################################################################################## 

a. Regression Analysis

Here we run create a logistic regression with response variable being the approved/denied column and the using all other variables as co-variates. Note the we use the shorthand formula notation '.' for including all other variables in the formula.

```{r}
simple_reg = glm(sale_def ~ bed + bath + area + area_heated + dist_cbd + dist_lakes + pool + lake_front + downtown + luxury, data=prop_prices)
summary(simple_reg) 
coef(simple_reg) 

```

Which of the variables tested significant at the 95% level? Looking at the results and answering outside of the chunk is sufficient.

Answer: Pool tested significant at the 95% confidence level.

## Evaluating the model

As is, are any of the Gauss-Markov assumptions violated? If so, which ones? How can you fix the issues?

Using the plot of the regression model to test for Guass-Markov violations.

The Residuals vs Fitted plot shows that there is increasing variance as price increases indicating bias which violates assumption 1. This plot also show that there is no linearity which is also a violation.

The Q-Q plot does not show a diagnol line indicating that the data is not normally distributed.

```{r}

plot(simple_reg)

```

## New Model

Based off of your findings in the previous section, make changes to the variables, the functional form, etc.

```{r}
# Try removing outliers
normalized <- subset(prop_prices, prop_prices$sale_def < 1000000)

# Use only the most significant variables in the formula
fixed_reg = glm(sale_def ~ bed + area_heated + lake_front + luxury, data=normalized)
summary(fixed_reg) 
coef(fixed_reg) 
plot(fixed_reg)
# Looking at the plots, it seems like a better fit

# Try using log of large values to even the playing field
fixed_reg2 = glm(log(sale_def) ~ bed + log(area_heated) + lake_front + luxury, data=normalized)
summary(fixed_reg2) 
coef(fixed_reg2) 
plot(fixed_reg2)

```

# Prediction

Based on the following inputs, predict the deflated sales price:

-   2 bed
-   2 bath
-   area_heated = 1223
-   area = 9750
-   dist_cbd = 19368
-   dist_lakes = 490
-   no pool

```{r}
# Using basic model
new_house_df <- data.frame(bed=2,bath=2,area_heated=1223,area=9750,dist_cbd=19368,dist_lakes=490,pool=0,lake_front=0,luxury=0)
basic_linear_model <- lm(formula = sale_def ~ bed + bath + area + area_heated + dist_cbd + 
    dist_lakes + pool, data = prop_prices)
predict(basic_linear_model, newdata=new_house_df)

# Now using fixed models
predict(fixed_reg, newdata=new_house_df)

# Need to take e^predication since fixed_reg2 is based on logs
exp(predict(fixed_reg2, newdata=new_house_df))


```
